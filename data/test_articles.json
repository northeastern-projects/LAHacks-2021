{"Articles": [{"title": "High-resolution millimeter-wave imaging for humans", "description": "The use of networking signals has been extended beyond communication to sensing, localization, robotics and autonomous systems in recent years. Specifically, recent advances in 5G millimeter wave (mmWave) have explored the possibility of expanding the use of mmWave beyond device communications and simple range sensing to a full-fledged imaging under low visibility conditions (fog, smog, snow, etc.). This thesis explores the use of mmWave imaging for humans, which could be incorporated into autonomous driving technology for pedestrian imaging in low visibility conditions. Unfortunately, certain challenges have been identified in mmWave imaging for humans, including its low resolution, the presence of fake artifacts resulting from multipath reflections, and the vibration of the human body. This thesis presents a system that can enable high-resolution mmWave imaging for humans that tries to address the above challenges by leveraging recent advances in deep learning, known as generative adversarial networks (GANs). In this thesis, we propose a GAN architecture that is customized to mmWave imaging and build a system that can significantly enhance the quality of mmWave images for humans.U of I Onlyundergraduate senior thesis not recommended for open acces", "position": {"x": -0.028330583947402323, "y": 0.43196719640245923, "z": 0.3306031429737353}, "dissimilarity": [0.0, 3.129317283630371, 3.4501657485961914, 4.975581645965576, 4.379672050476074, 3.2166807651519775, 4.380445957183838, 5.567908763885498, 4.078155517578125, 3.4632205963134766, 3.110740900039673, 3.676098108291626, 3.1312997341156006, 3.0920186042785645, 4.679551124572754, 5.036942005157471, 3.892503261566162, 4.727444648742676, 3.257836103439331, 3.58870530128479, 3.568483829498291, 3.3595471382141113, 4.551424026489258, 4.72881555557251, 3.8389158248901367, 5.345144748687744, 3.674208164215088, 3.14943265914917, 4.310851573944092, 5.9832892417907715, 4.288661003112793, 4.320152759552002, 4.826199531555176, 3.5978641510009766, 3.749558448791504, 4.219302654266357, 4.307633399963379]}, {"title": "Predicting effect of force using game engine synthesized dataset", "description": "This research aims to solve the problem of predicting the long-term sequential movements of an object in an image given a force vector applied to the object. To approach this topic using deep learning method, a dataset rich in quality and quantity is desired. In this thesis, a method of generating the dataset is proposed and evaluated. A game engine (Unity 3D) is used to generate a dataset with scene screenshots, object mask, force representation and resulting movements. With one scene, a total of 370,440 sequences can be generated. The result shows that, with a larger and more precise dataset, the accuracy can increase significantly.U of I Onlyundergraduate senior research not recommended for open acces", "position": {"x": 0.7049522136918497, "y": 0.9812600913745299, "z": 0.23606862185434047}, "dissimilarity": [3.129317283630371, 0.0, 3.7320029735565186, 4.909344673156738, 4.610052585601807, 3.286001443862915, 4.42813777923584, 5.5763983726501465, 4.230092525482178, 3.386749029159546, 3.177102565765381, 3.9286911487579346, 2.9873604774475098, 3.459747076034546, 4.568272590637207, 5.378756046295166, 3.454049587249756, 4.574948310852051, 3.4521231651306152, 3.4365766048431396, 3.668550491333008, 3.271277666091919, 4.7275567054748535, 4.926558494567871, 3.6118617057800293, 5.44033670425415, 3.431434154510498, 3.8795783519744873, 4.135000228881836, 6.055348873138428, 4.380533695220947, 4.128567695617676, 4.772590160369873, 3.60066294670105, 3.7910945415496826, 3.9704675674438477, 4.546658515930176]}, {"title": "Deep Learning of Sleep Quality based on Ballistocardiographic sensors, Stigmergic Perceptrons and LSTM Networks", "description": "The negative effects due to inadequate sleep in human beings of any age are well known. Neuroscientists and sleep experts work every day to understand how and what makes sleep more effective in its physical and mental recovery action. Among the most adopted techniques, the standard for the measurement of vital parameters on sleeping subjects is certainly Polysomnography. It is a method that involves many sensors and a laboratory environment, factors that introduce inconveniences that could negatively influence the sleep of the individual himself. In this thesis we will base the experimentation on signals obtained through Ballistocardiography, a portable and less intrusive technique that deduces the heartbeat and respiratory acts based on the accelerations of the body lying on the bed due to forces imparted by the heart to the mass of blood that is pumped to the peripheral body systems. In order to solve the problem of the parametric complexity of explicit analytical models of sleep quality in terms of \"sleep architecture\", in this work we propose a Deep Learning architecture based on multiple levels of Stigmergic Perceptrons. This approach describes a soft classification technique on time series with respect to a collection of archetypes, each representing a different behavioral class. Finally, the Deep Learning architecture is integrated with recurrent Long Short-Term Memory networks, known in literature for the classification of time series, above all for their ability to solve the problem of long-term dependencies over time. This technology is appropriate in order to establish a classifier able to recognize the level of quality of sleep declared by the subjects of the measurements and confirmed by a result obtained through the application of quantitative heuristics. Keywords: Deep Learning, Sleep Quality, Ballistocardiography, Stigmergic Perceptron, LSTM network [Italian] Sono molto noti gli effetti negativi dovuti ad un sonno non adeguato in esseri umani di qualiasi et\u00e0. Neuroscienziati ed esperti del sonno si adoperano ogni giorno per capire come e cosa renda il sonno pi\u00f9 efficace nella sua azione di recupero fisico e mentale. Tra le tecniche pi\u00f9 adottate, lo standard per le misurazioni dei parametri vitali su soggetti addormentati \u00e8 sicuramente la Polisonnografia. Si tratta di un metodo che prevede molti sensori e un ambiente di laboratorio, fattori che introducono scomodit\u00e0 che potrebbero influenzare negativamente il sonno dell'individuo stesso. In questa tesi baseremo la sperimentazione su segnali ottenuti tramite Ballistocardiografia, una tecnica portabile e meno intrusiva che deduce il battito cardiaco e gli atti respiratori in base alle accelerazioni del corpo giacente sul letto dovute a forze impartite dal cuore alla massa di sangue che viene pompata ai sistemi periferici del corpo. Al fine di risolvere il problema della complessit\u00e0 parametrica dei modelli analitici espliciti della qualit\u00e0 del sonno in termini di \"architettura del sonno\", in questo lavoro si propone una architettura di Apprendimento Profondo basata su pi\u00f9 livelli di Percettroni Stigmergici. Questo approccio descrive una tecnica di classificazione soft su serie temporali rispetto ad una raccolta di archetipi, ciascuno rappresentante una differente classe comportamentale. In fine, all'architettura di Apprendimento Profondo si integra l'uso di reti ricorrenti Long Short-Term Memory, note in letteratura per la classificazione di serie temporali soprattutto per la loro capacit\u00e0 di risolvere il problema delle dipendenze a lungo termine nel tempo. Questa tecnologia si rende opportuna allo scopo di costituire un classificatore in grado di riconoscere il livello di qualit\u00e0 del sonno dichiarato dai soggetti delle misurazioni e confermato da un risultato ottenuto tramite applicazione di euristiche quantitative. Parole chiave: Apprendimento profondo, Qualit\u00e0 del sonno, Ballistocardiografia, Percettrone Stigmergico, reti LST", "position": {"x": -1.2760456661128652, "y": -0.2980306768977303, "z": -1.2848459822314164}, "dissimilarity": [3.4501657485961914, 3.7320029735565186, 0.0, 4.519571304321289, 4.195614337921143, 4.00416898727417, 4.048891067504883, 5.124899387359619, 4.600017070770264, 3.7008681297302246, 3.940962791442871, 4.836287021636963, 3.8647027015686035, 3.693969249725342, 4.138565540313721, 4.892369747161865, 4.920700550079346, 3.5145273208618164, 3.4897797107696533, 4.037565231323242, 4.354455471038818, 4.472334861755371, 4.205778121948242, 4.5597639083862305, 3.504824161529541, 5.099156379699707, 4.6400628089904785, 3.9049673080444336, 3.840690851211548, 5.464784622192383, 3.8064639568328857, 4.020384311676025, 4.755273342132568, 3.9850220680236816, 3.4604220390319824, 4.461506366729736, 3.6839065551757812]}, {"title": "Sviluppo di reti convolutive profonde per la stima dell'et\u00e0 cardiaca da immagini CT del torace", "description": "La Convolutional neural network (CNN), uno dei metodi di deep learning pi\u00f9 comunemente utilizzato, \u00e8 stata applicata a vari tasks di computer vision e pattern recognition, e ha raggiunto prestazioni all'avanguardia. I pi\u00f9 recenti lavori di ricerca sulla CNN si concentrano sulle innovazioni della struttura. Questo lavoro di tesi esamina e implementa strutture innovative di CNN in grado di stimare l\u2019et\u00e0 cardiaca da immagini CT del torace. Per eseguire tale compito, si propone un convolutional autoencoder 3D, modello particolarmente adatto per l\u2019estrazione di features profonde dalle immagini. \u00c8 noto infatti che features anatomiche del cuore, come le dimensioni dell\u2019organo, calcificazioni del tessuto miocardico, sono in relazione con l\u2019et\u00e0 cronologica di un individuo. Successivamente si applica una tecnica di transfer learning per trasferire le informazioni anatomiche a una CNN e abilitarla al task di regressione sull\u2019et\u00e0", "position": {"x": 1.525554388548911, "y": -2.639645554065393, "z": -1.9975989810725965}, "dissimilarity": [4.975581645965576, 4.909344673156738, 4.519571304321289, 0.0, 3.92724871635437, 5.607302188873291, 5.269209861755371, 2.1293277740478516, 5.091189861297607, 5.096383094787598, 5.093291282653809, 5.300291538238525, 5.223310470581055, 5.280167102813721, 5.630379676818848, 6.031303405761719, 5.561304569244385, 5.953194618225098, 5.43565034866333, 5.3923659324646, 5.048092365264893, 4.982318878173828, 5.52103328704834, 3.7925686836242676, 5.525007247924805, 4.143187046051025, 5.088454246520996, 5.21055793762207, 5.595180511474609, 2.895159959793091, 5.0667009353637695, 5.143200874328613, 5.57895040512085, 5.623946666717529, 4.075807571411133, 5.222973823547363, 5.341111183166504]}, {"title": "Artificial intelligence within the interplay between natural and artificial computation: Advances in data science, trends and applications", "description": "Artificial intelligence and all its supporting tools, e.g. machine and deep learning in computational intelligence-based systems, are rebuilding our society (economy, education, life-style, etc.) and promising a new era for the social welfare state. In this paper we summarize recent advances in data science and artificial intelligence within the interplay between natural and artificial computation. A review of recent works published in the latter field and the state the art are summarized in a comprehensive and self-contained way to provide a baseline framework for the international community in artificial intelligence. Moreover, this paper aims to provide a complete analysis and some relevant discussions of the current trends and insights within several theoretical and application fields covered in the essay, from theoretical models in artificial intelligence and machine learning to the most prospective applications in robotics, neuroscience, brain computer interfaces, medicine and society, in general.BMS - Pfizer(U01 AG024904). Spanish Ministry of Science, projects: TIN2017-85827-P, RTI2018-098913-B-I00, PSI2015-65848-R, PGC2018-098813-B-C31, PGC2018-098813-B-C32, RTI2018-101114-B-I, TIN2017-90135-R, RTI2018-098743-B-I00 and RTI2018-094645-B-I00; the FPU program (FPU15/06512, FPU17/04154) and Juan de la Cierva (FJCI-2017\u201333022). Autonomous Government of Andalusia (Spain) projects: UMA18-FEDERJA-084. Conseller\u00eda de Cultura, Educaci\u00f3n e Ordenaci\u00f3n Universitaria of Galicia: ED431C2017/12, accreditation 2016\u20132019, ED431G/08, ED431C2018/29, Comunidad de Madrid, Y2018/EMT-5062 and grant ED431F2018/02.  PPMI \u2013 a public \u2013 private partnership \u2013 is funded by The Michael J. Fox Foundation for Parkinson\u2019s Research and funding partners, including Abbott, Biogen Idec, F. Hoffman-La Roche Ltd., GE Healthcare, Genentech and Pfizer Inc", "position": {"x": 0.32726729485362877, "y": -3.1148999872728838, "z": 0.6054368771790695}, "dissimilarity": [4.379672050476074, 4.610052585601807, 4.195614337921143, 3.92724871635437, 0.0, 5.130441665649414, 4.816651344299316, 4.379864692687988, 4.3358869552612305, 4.27470064163208, 4.668459892272949, 4.954821586608887, 4.975893497467041, 4.693498611450195, 5.44457483291626, 5.3763227462768555, 5.203094005584717, 5.631091594696045, 4.529058933258057, 5.00469446182251, 4.84700870513916, 4.7919464111328125, 5.025503158569336, 4.229608535766602, 5.139072895050049, 3.2826762199401855, 4.769284248352051, 4.603420734405518, 5.399021625518799, 4.771729946136475, 4.396870136260986, 4.8603105545043945, 5.1127424240112305, 5.0475754737854, 3.859713315963745, 4.649730682373047, 4.871465682983398]}, {"title": "Certified adversarial robustness via randomized discretization", "description": "Modern machine learning algorithms are able to reach an astonishingly high level of performance in a variety of useful tasks. However, small adversarial perturbations have been shown to drastically reduce the accuracy of deep learning models not specifically trained to resist them. This problem is of practical significance due to security concerns about models deployed in industry, and of theoretical significance due to the connections between this problem and the underlying themes of optimization and generalization. In this paper, we propose and analyze a simple and computationally efficient defense against adversarial attacks based on randomized discretization to a relatively small set of points that is agnostic of the underlying classifier. We show that that this strategy leads to a lower bound on the classification accuracy using tools from computational geometry and information theory. Unlike prior work, the proposed strategy allows for easily estimable  data-dependent accuracy guarantees at inference time, and demonstrates a weaker dependence on the dimensionality of its inputs.U of I Onlyundergraduate senior thesis not recommended for open acces", "position": {"x": -0.5787781232612883, "y": 2.07968004045482, "z": 2.1617968359331785}, "dissimilarity": [3.2166807651519775, 3.286001443862915, 4.00416898727417, 5.607302188873291, 5.130441665649414, 0.0, 4.879580974578857, 6.1021928787231445, 4.5189127922058105, 3.6494898796081543, 3.2195489406585693, 3.68300724029541, 3.265390634536743, 3.5645689964294434, 5.064455986022949, 5.493101596832275, 4.459878444671631, 4.851315975189209, 3.1492857933044434, 2.7331831455230713, 3.7133994102478027, 3.6498076915740967, 5.031515121459961, 5.6485466957092285, 4.145291805267334, 6.194562911987305, 3.8319358825683594, 3.6200246810913086, 4.3480544090271, 6.562702655792236, 4.549394130706787, 4.785196781158447, 5.391983985900879, 2.6502113342285156, 4.277914047241211, 4.790546417236328, 4.503316879272461]}, {"title": "Prostate cancer diagnosis with deep learning", "description": "Prostate cancer is one of the most common cancers and the second leading cause of death among American men. However, prostate cancer diagnosis is one of the most urgent problems confronted by scientific research.  Accurate prostate cancer diagnosis needs a great degree of medical knowledge and is usually based on experience. It is hard for ordinary men to diagnose prostate cancer by themselves. This project aims to eliminate the  knowledge  barrier and provide a precise and effective method using deep learning. This project uses a deep learning neural network to build a binary classifier for prostate needle biopsies from patients. The project enlarges the prostate cancer needle biopsies dataset using randomly cutting, builds the deep learning  network binary classifier, and generates predictions for the biopsies. The classifier will assign a benign or malignant label to every biopsy with accuracy near 100%.U of I Onlyundergraduate senior thesis not recommended for open acces", "position": {"x": -0.7783210048725763, "y": 0.7956612737655159, "z": -3.0847822361931607}, "dissimilarity": [4.380445957183838, 4.42813777923584, 4.048891067504883, 5.269209861755371, 4.816651344299316, 4.879580974578857, 0.0, 5.853219985961914, 5.091580867767334, 4.8307576179504395, 4.4395833015441895, 4.935570240020752, 4.705473899841309, 4.744307994842529, 4.232171058654785, 5.112048149108887, 5.286190986633301, 4.446403980255127, 4.81038236618042, 4.888014793395996, 4.768709659576416, 4.726858139038086, 4.745466709136963, 5.272087097167969, 4.33198881149292, 5.563389301300049, 4.823392868041992, 4.99324893951416, 3.6681747436523438, 6.052890777587891, 4.660665512084961, 4.9445061683654785, 4.881715297698975, 4.95728063583374, 4.958622455596924, 5.1520304679870605, 4.335175037384033]}, {"title": "Leveraging Deep Learning for Automated Image Anonymization in the Insurance Domain", "description": "Sommario Questo lavoro di tesi \u00e8 il risultato di uno stage della durata di cinque mesi, inserito all\u2019interno del programma Junior Consulting promosso da ELIS Consulting & Labs. Durante questo periodo, la candidata ha preso parte ad un progetto di consulenza finalizzato all\u2019utilizzo degli strumenti di Image Analytics per la corretta gestione dei dati sensibili che vengono processati in ambito assicurativo, in conformit\u00e0 con la normativa Europea GDPR. L\u2019obiettivo del progetto \u00e8 stato quello di realizzare un algoritmo di Object Detection ed anonimizzazione di persone, targhe e numeri di telaio presenti in immagini, basato sui concetti di apprendimento supervisionato delle reti neurali profonde. La componente tecnologica dell\u2019algoritmo \u00e8 stata prodotta utilizzando il framework TensorFlow, che ha consentito di implementare una versione del modello RetinaNet in linea con le specifiche del task di anonimizzazione che \u00e8 stato richiesto dal committente di progetto. Il lavoro svolto durante l\u2019attivit\u00e0 progettuale ha fornito lo spunto per lo svolgimento di una ulteriore analisi. L\u2019oggetto dell\u2019indagine ha riguardato le possibili modalit\u00e0 di sfruttamento dei pi\u00f9 recenti strumenti di Intelligenza Artificiale nei processi dell\u2019industria assicurativa, in modo tale da supportare una strategia di trasformazione digitale che dipende largamente dalla corretta gestione ed interpretazione dei dati. I risultati ottenuti dall\u2019analisi della letteratura in merito evidenziano come l\u2019implementazione di strumenti basati su modelli di machine learning e deep learning possa consentire alle aziende del settore assicurativo di migliorare i processi basati sull\u2019analisi di dati strutturati e non strutturati (come le immagini). Tale miglioramento \u00e8 potenzialmente quantificabile mediante l\u2019incremento delle metriche di performance legate alla soddisfazione del cliente nel medio/lungo termine. Abstract This thesis work is the result of a five-month internship, included in a talent-program called Junior Consulting promoted by ELIS Consulting & Labs, in Rome. During this period, the candidate took part in a consulting project aimed at using Image Analytics tools for the proper management of sensitive data which are usually processed in insurance companies\u2019 business processes, in order to be compliant with the GDPR European regulation. In this respect, the need has emerged for the implementation of an Object Detection algorithm, also aimed to remove license plates, Vehicle Identification Numbers (VIN), and persons from images used for insurance purposes, by relying on the supervised learning of deep neural networks. The technological part was realized using the TensorFlow framework, which allowed to implement a customized RetinaNet model in line with the anonymization task requirements that were specifically sought by the project client. In addition, the work carried out during the project provided the basis for further context analysis. The object of this additional survey concerns the possible ways of leveraging the most recent artificial intelligence tools in insurance business processes, in order to support a digital transformation strategy focused on value deriving from meaningful interpretation and management of data. The results which were obtained from the literature review show how the implementation of machine learning-based and deep learning-based tools allow insurance companies to improve their data-driven processes, unlocking value from the analysis of both structured and unstructured data (e.g., images). The benefits of this approach are expected to be measured in the medium/long run, in terms of positive impact on the customer satisfaction key performance indicators in the insurance domain", "position": {"x": 1.9219228443105139, "y": -3.542726092207395, "z": -1.9310353452554938}, "dissimilarity": [5.567908763885498, 5.5763983726501465, 5.124899387359619, 2.1293277740478516, 4.379864692687988, 6.1021928787231445, 5.853219985961914, 0.0, 5.6047773361206055, 5.617285251617432, 5.707379341125488, 5.856334686279297, 5.783505439758301, 5.867367744445801, 6.120215892791748, 6.52542781829834, 6.181286334991455, 6.560975551605225, 5.898297309875488, 5.9703898429870605, 5.491211891174316, 5.64859676361084, 5.963654518127441, 3.944782018661499, 6.175036430358887, 4.336590766906738, 5.724910736083984, 5.640610218048096, 6.151061534881592, 2.6285831928253174, 5.411446571350098, 5.7675299644470215, 6.073052406311035, 6.105893135070801, 4.4727630615234375, 5.563938140869141, 5.821974754333496]}, {"title": "Industrial Strength Multilingual Named Entity Collection for the SPIRIT Project", "description": "Scalable privacy preserving intelligence analysis for resolving identities (SPIRIT) is a Cybersecurity and OSINT project which has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 786993. The SPIRIT Project aims to provide an innovative platform based on an integrated approach for the multimodal and multilingual analysis of different types of content from different types of sources in order to facilitate cognitive tasks in the resolution of criminal identities. In the present work we give a detailed overview of the SPIRIT platform's architecture and pipelines, showing the entire technological stack. Then, we will shift the attention to the NLP and Deep Learning techniques adopted and the experiments conducted in order to provide a performing Multilingual Open Domain Named Entity Recognition engine", "position": {"x": 3.1330097243683497, "y": -0.3247955944271982, "z": 0.30214368103469663}, "dissimilarity": [4.078155517578125, 4.230092525482178, 4.600017070770264, 5.091189861297607, 4.3358869552612305, 4.5189127922058105, 5.091580867767334, 5.6047773361206055, 0.0, 4.567043781280518, 3.9435291290283203, 4.211255073547363, 4.486273288726807, 4.132339954376221, 5.406764030456543, 5.730576038360596, 3.835231304168701, 5.217075824737549, 4.205384731292725, 4.652491569519043, 3.7690627574920654, 3.7394769191741943, 5.146759986877441, 5.057838439941406, 4.758658409118652, 5.2574567794799805, 3.4958655834198, 4.1098103523254395, 5.26132869720459, 6.005805492401123, 4.831540107727051, 5.331027030944824, 5.249598026275635, 4.382742881774902, 4.61323356628418, 4.747995376586914, 5.054264545440674]}, {"title": "Build and train advanced model for image generation", "description": "Recent developments on deep learning have enabled generative models to capture distribution of  relatively complex datasets. In this research we aim to build a   cutting-edge model that is able to  learn the distribution of the CelebA face dataset. We surveyed several papers published in recent years  and decided to construct and improve the VAE-GAN model, which combines the Variational Autoencoder (VAE)  and Generative Adversarial Network (GAN). Chapter 1 introduces GAN, VAE, VAE-GAN model and  addresses a disadvantage of GAN and proposes some methods to improve it. We also introduce  Adversarial Autoencoder which we intend to accompany the model in the future. The other chapters  address some details of  implementation and show the results of the experiment.U of I OnlyUndergraduate senior thesis not recommended for open acces", "position": {"x": -1.8808010393047772, "y": -0.6428065243664304, "z": 1.5559084675936061}, "dissimilarity": [3.4632205963134766, 3.386749029159546, 3.7008681297302246, 5.096383094787598, 4.27470064163208, 3.6494898796081543, 4.8307576179504395, 5.617285251617432, 4.567043781280518, 0.0, 3.5226683616638184, 4.542657852172852, 3.4877946376800537, 3.9250080585479736, 5.083867073059082, 5.434945106506348, 4.654226303100586, 5.035093784332275, 3.346632957458496, 3.5629234313964844, 4.422765731811523, 4.010089874267578, 4.775719165802002, 4.912823677062988, 4.160906791687012, 5.120466709136963, 4.304684162139893, 3.847090482711792, 3.9632084369659424, 5.994634628295898, 4.467881202697754, 4.270042419433594, 4.70551061630249, 3.397017002105713, 3.7143914699554443, 3.7824809551239014, 4.278541564941406]}, {"title": "Privacy in distributed machine learning", "description": "This research explores ways to effectively use distributed machine learning while preserving privacy. Distributed learning was done on a client-server architecture where each client is an individual learner training on his or her own dataset and the server exchanges gradients between learners. Each learner used a convolutional neural network to learn the MNIST dataset (handwritten digits). Only gradients are exchanged instead of training data resulting in some privacy, but additional privacy was added by multiplying random weights to these gradients. The first step of the research was to replicate results from Shokri and Shmatikov\u2019s 2015 paper on privacy-preserving deep Learning. Shokri and Shmatikov\u2019s paper primarily worked with multiple clients and a single server. The research extends this architecture by creating a multiple server and multiple client architecture and adding random weights to the gradients. The research successfully produced empirical results showcasing the effectiveness of our distributed machine learning algorithm since it was able to maintain an acceptable classification accuracy while providing a certain degree of privacy. In addition to Shokri and Shmatikov\u2019s paper, this research draws upon Gade and Vaidya\u2019s 2016 paper \u201cDistributed Optimization for Client-Server Architecture with Negative Gradient Weights\u201d.U of I OnlyUndergraduate senior thesis not recommended for open acces", "position": {"x": 0.08514140652366012, "y": -0.36186578193035396, "z": 1.7007310691640278}, "dissimilarity": [3.110740900039673, 3.177102565765381, 3.940962791442871, 5.093291282653809, 4.668459892272949, 3.2195489406585693, 4.4395833015441895, 5.707379341125488, 3.9435291290283203, 3.5226683616638184, 0.0, 3.7326695919036865, 3.501260995864868, 3.7473955154418945, 4.508997440338135, 5.567782878875732, 3.6939504146575928, 4.628945350646973, 3.2917978763580322, 3.4720394611358643, 3.440725326538086, 3.425191640853882, 4.693143367767334, 4.625190734863281, 3.828252077102661, 5.366824150085449, 3.202246904373169, 4.160487651824951, 4.156991004943848, 6.135010242462158, 4.132500648498535, 4.067930698394775, 5.125859260559082, 3.3912997245788574, 3.6512253284454346, 3.9704084396362305, 4.493851184844971]}, {"title": "Denial of service exploits on CUDA devices using clock function", "description": "NVIDIA\u2019s CUDA devices are increasingly being used in applications from accelerating computer graphics to deep learning and numerical analysis. Because the purpose of GPUs is to accelerate a diverse set of applications, it is important for them to execute code as fast as possible with the highest degree of throughput. Unlike CPUs which have been targets of security exploits since their inception (many decades ago) GPUs have only been available for a diverse set of applications and widely adopted in the last decade. Due to GPUs relative novelty and increasing adoption by industry and government alike, there has been an increased interest in security vulnerabilities and their corresponding solutions. In this thesis we uncover a vulnerability with the library clock function that can cause device-wide slowdown on Unix-based systems, impact performance and, in some cases, cause total system failure. Different generations of CUDA devices running on different operating systems were benchmarked. The disturbances in the benchmark output demonstrated consistent vulnerabilities in Unix systems and some occasional problems with Windows systems. Two potential methods to detect and resolve malicious programs have been prototyped on CPUs in this study and are currently in development to be ported to GPUs.U of I Onlyundergraduate senior thesis not recommended for open acces", "position": {"x": 1.477009070459502, "y": 0.6440167633353293, "z": 3.046230970041972}, "dissimilarity": [3.676098108291626, 3.9286911487579346, 4.836287021636963, 5.300291538238525, 4.954821586608887, 3.68300724029541, 4.935570240020752, 5.856334686279297, 4.211255073547363, 4.542657852172852, 3.7326695919036865, 0.0, 4.143428802490234, 4.13442850112915, 5.272964000701904, 5.378249168395996, 4.416823387145996, 5.539573669433594, 3.8872456550598145, 3.6184537410736084, 3.3964993953704834, 2.9507863521575928, 5.289665222167969, 5.323109149932861, 4.7408599853515625, 5.838648796081543, 3.398285150527954, 4.059154987335205, 5.027292251586914, 6.150172233581543, 5.004019260406494, 5.164353370666504, 5.400923252105713, 4.184877395629883, 4.900423049926758, 4.167258262634277, 5.263599872589111]}, {"title": "Very Low-Quality Recognition Using Conventional Neural Network: With an Application to Face Identification", "description": "Visual recognition from very low-quality images is an extremely challenging task with great practical values, due to the ubiquitous existence of quality distortions during image acquisition, transmission, or storage. While deep networks have been extensively applied to low-quality image restoration and high-quality image recognition tasks respectively, less has been done on the important problem of recognition from very low-quality images. I propose a degradation-robust pre-training method to jointly tune reconstruction and classification with comprehensive analysis on improving deep learning models along this direction. This jointly tuning leverages the power of pre-training similar to that of transfer learning and generalizes conventional unsupervised pre-training and data augmentation methods. I did extensive experiments on a number of diverse real-world datasets to validate the effectiveness of the proposed method and applied this method on face identification tasks.U of I OnlyUndergraduate senior thesis not recommended for open acces", "position": {"x": -1.283253164446578, "y": 1.9176057088677516, "z": 0.952838578987846}, "dissimilarity": [3.1312997341156006, 2.9873604774475098, 3.8647027015686035, 5.223310470581055, 4.975893497467041, 3.265390634536743, 4.705473899841309, 5.783505439758301, 4.486273288726807, 3.4877946376800537, 3.501260995864868, 4.143428802490234, 0.0, 3.700535297393799, 4.504144668579102, 5.708069324493408, 4.23682975769043, 4.510400772094727, 3.4866013526916504, 3.7584569454193115, 3.909933090209961, 3.4762120246887207, 4.993160724639893, 5.278724193572998, 3.8246943950653076, 5.789344787597656, 3.8647656440734863, 3.6444952487945557, 3.9806840419769287, 6.211008548736572, 4.662146091461182, 4.223714828491211, 5.195328712463379, 3.5192716121673584, 4.251342296600342, 4.114663600921631, 4.342266082763672]}, {"title": "Car Detection using Unmanned Aerial Vehicles: Comparison between Faster R-CNN and YOLOv3", "description": "Unmanned Aerial Vehicles are increasingly being used in surveillance and traffic monitoring thanks to their high mobility and ability to cover areas at different altitudes and locations. One of the major challenges is to use aerial images to accurately detect cars and count-them in real-time for traffic monitoring purposes. Several deep learning techniques were recently proposed based on convolution neural network (CNN) for real-time classification and recognition in computer vision. However, their performance depends on the scenarios where they are used. In this paper, we investigate the performance of two state-of-the art CNN algorithms, namely Faster R-CNN and YOLOv3, in the context of car detection from aerial images. We trained and tested these two models on a large car dataset taken from UAVs. We demonstrated in this paper that YOLOv3 outperforms Faster R-CNN in sensitivity and processing time, although they are comparable in the precision metric.info:eu-repo/semantics/publishedVersio", "position": {"x": 0.9707243627752962, "y": 2.1684322107654843, "z": -0.4740402418540577}, "dissimilarity": [3.0920186042785645, 3.459747076034546, 3.693969249725342, 5.280167102813721, 4.693498611450195, 3.5645689964294434, 4.744307994842529, 5.867367744445801, 4.132339954376221, 3.9250080585479736, 3.7473955154418945, 4.13442850112915, 3.700535297393799, 0.0, 4.698117256164551, 4.896866798400879, 4.065285682678223, 4.572083473205566, 3.6964986324310303, 3.9996163845062256, 4.093939781188965, 3.717194080352783, 4.609792709350586, 5.013948440551758, 3.3783481121063232, 5.660245895385742, 3.808417797088623, 3.332350254058838, 4.105634689331055, 6.3176140785217285, 4.937013149261475, 4.9839630126953125, 4.399770736694336, 3.8507657051086426, 4.127871036529541, 4.412691593170166, 4.271213531494141]}, {"title": "An assessment of automatic segmentation of the knee joint based on machine learning", "description": "The creation of an accurate 3D model of the bone and joint is a crucial requirement for any CAOS system, and the basis for any and all surgical planning. This model can be obtained by using image-based or image-free techniques. Image-based techniques rely on the segmentation of pre-operative scans. Segmentation is typically done manually, which is a time-consuming process that requires technical expertise. Image-free techniques were developed to avoid this preoperative step and rely on intraoperative surface scanning and bone morphing algorithms to create a patient-specific model during surgery. Recent advances in machine learning have aided the development of novel segmentation algorithms, which may eventually enable automation of the segmentation process in image-based techniques. In this work, we assessed the accuracy of digital models of a cadaveric femur obtained using three different systems (manual segmentation, a state-of-the-art machine learning algorithm and a commercial image-free surgical system), by comparing them to high resolution optical scans of the exposed femur, which is used here as the gold standard. One cadaveric knee was used for this study. Ethical approval was obtained from the Imperial College Healthcare Tissue Bank (project R13066-3A) and the cadaveric knee was sourced from an approved supplier. The intact sample was scanned using a Siemens Spectra MRI system to obtain the images needed for the image-based procedures. Manual segmentation of the femur bone and cartilage was performed in 3D-Slicer by thresholding, followed by manual adjustment. A 3D model of the segmented volume was then generated using the same software. The automated segmentation was performed using the Convolutional Neural Network (CNN) architecture designed by Kayalibay et al. The neural network was trained to classify voxels into five categories (femur bone, femur cartilage, tibia bone, tibia cartilage, other) using 70 MRI volumes of the knee, available from the SKI10 challenge. Training required approximately 41 hours on an NVIDIA Tesla K80 Graphic Processing Unit (GPU). After training, the same neural network was used to segment an MRI scan of the specimen. The label maps obtained from the network were then converted into a 3D model using Matlab R2018a. The knee specimen was prepared for optical surface scanning by attaching it to a custom-made metal rig using bone cement and exposing the distal end of the femur by performing a vertical cut as in a Total Knee Arthroplasty (TKA). To obtain a gold standard measurement, 12 high resolution scans of the visible part of the femur head were obtained from different positions and orientations, using a Polyga HDI C210 3D Scanner. The scans were then aligned and merged using the FlexScan3D software, available from the camera manufacturer. The specimen was then digitised using a commercial image-free surgical system. To this end, a reference array was attached to the femur using bone pins and, after standard calibration, the visible part of the femur was digitised using the system\u2019s probe. After this, the system\u2019s log files were downloaded to a USB drive to access the computed surface model. The 3D models obtained with the different modalities were imported into Matlab as point clouds and were registered to the gold standard using the Iterative Closest Point (ICP) method. To summarise the accuracy of each method, the root mean square error (RMSE) was computed for each modality. Manual segmentation is confirmed here to be the most accurate method, but it requires expertise and time. Hence, in order to alleviate the task for clinicians, image-free systems have gained in popularity. Our results show that the accuracy of a state-of-the-art machine-learning algorithm is still worse than with other methods, but inching towards the ballpark figure for image-free systems, albeit with the need for preoperative imaging of the patient, which would add time and cost to a navigated procedure. These results suggest that machine-learning methods could become a powerful alternative to currently available segmentation techniques. With the attention that these methods have been receiving in recent years, and the rapid development of hardware and software tools for deep learning, the quality and speed of execution of these methods will continue to improve in the coming years, with a consequent impact on their speed, robustness, and overall accuracy", "position": {"x": -2.397722363131426, "y": 0.8163618349544286, "z": -2.6568996443593167}, "dissimilarity": [4.679551124572754, 4.568272590637207, 4.138565540313721, 5.630379676818848, 5.44457483291626, 5.064455986022949, 4.232171058654785, 6.120215892791748, 5.406764030456543, 5.083867073059082, 4.508997440338135, 5.272964000701904, 4.504144668579102, 4.698117256164551, 0.0, 6.0402021408081055, 4.8866868019104, 3.638519525527954, 5.000703811645508, 5.247060775756836, 5.025208473205566, 4.746350288391113, 5.374106407165527, 5.2130327224731445, 3.954335927963257, 5.915543556213379, 4.726866722106934, 5.021250247955322, 3.5800249576568604, 6.408110618591309, 5.124495506286621, 5.137357711791992, 5.574424743652344, 5.100447654724121, 5.023474216461182, 4.935129642486572, 4.240714073181152]}, {"title": "Predicting the COVID-19 cases using deep learning", "description": "In an increasingly globalized modern society, where the life expectancy is keeping improving, humanity have to face an increasing threat: viruses. Viruses have always been part of the human life, however the evolution of the society towards a more globalized one has meant that the spread of these pathogens have become quicker. Many of such human pathogens have a mammalian and avian origin, when such transmission happens there is the so called spillover. Wild animals live together their own viruses from thousands of years and with evolution have developed an appropriate immune system to face them. The climatic change and the destruction of natural habitats increases the contact between wildlife and humans. This way, the spillover probability increases, exposing humans to unknown viruses with potentially catastrophic outcomes. Recent examples of such events are the filovirus Ebola (discovered for the first time in 1976 in Zaire), the coronavirus SARS (2003) and MERS (2012) and the bird flu. In recent years the rapid evolution of machine learning technologies have brought remarkable improvements in many fields. In particular, the time-series forecasting is an hot topic which covers many possible applications such as weather, houses and stock prices prediction. Today, the world is facing a new enemy: COVID19. Actually, there's not any knowledge about it's origin and how to defeat it. Researchers allover the world are trying to find a cure and a vaccine in order to eradicate it. However, being able to predict an epidemic diffusion is of strategic importance. Know in advance how many cases there might be in a given region could allow to anticipate it by reducing the spread and being able to provide better treatments to the infected", "position": {"x": 1.8012136389044942, "y": 3.2635340213518975, "z": -2.611986154421194}, "dissimilarity": [5.036942005157471, 5.378756046295166, 4.892369747161865, 6.031303405761719, 5.3763227462768555, 5.493101596832275, 5.112048149108887, 6.52542781829834, 5.730576038360596, 5.434945106506348, 5.567782878875732, 5.378249168395996, 5.708069324493408, 4.896866798400879, 6.0402021408081055, 0.0, 6.202635288238525, 5.699166774749756, 5.1186347007751465, 5.321616172790527, 5.581986904144287, 5.376788139343262, 5.468994617462158, 6.083600044250488, 4.732461452484131, 6.233067989349365, 5.81631326675415, 4.970805644989014, 5.4678826332092285, 6.563302993774414, 5.577978610992432, 5.929627895355225, 4.899244785308838, 5.598966598510742, 6.010293960571289, 5.7856645584106445, 5.657176494598389]}, {"title": "Small Obstacles Detection for Autonomous Navigation", "description": "The work focuses on an obstacle detection framework for an autonomous robotics platform. State of the art classical and Deep Learning existing frameworks and approaches are first explored. A probabilistic framework that includes 3D reconstruction techniques, stereo matching and UV-disparity projection is designed. The system is integrated with appearance cues, edges related metrics and ground plane homography estimation to robustly segment obstacles in a wide range of scenarios. A synthetic and real datasets are generated to test the performances and accuracy of the proposed algorithms. The framework is deployed on a autonomous mobile robot where real time performances are achieved, using CUDA computing capabilities of a NVIDIA Xavier embedded platform", "position": {"x": 3.0931003182972137, "y": 1.48866079224161, "z": 0.7131553119495049}, "dissimilarity": [3.892503261566162, 3.454049587249756, 4.920700550079346, 5.561304569244385, 5.203094005584717, 4.459878444671631, 5.286190986633301, 6.181286334991455, 3.835231304168701, 4.654226303100586, 3.6939504146575928, 4.416823387145996, 4.23682975769043, 4.065285682678223, 4.8866868019104, 6.202635288238525, 0.0, 5.18625020980835, 4.311297416687012, 4.7071661949157715, 4.273778915405273, 3.6901028156280518, 5.54579496383667, 5.118865013122559, 4.470975399017334, 5.9459686279296875, 2.7130956649780273, 4.300152778625488, 5.145490646362305, 6.58211612701416, 5.511565208435059, 5.410195350646973, 5.4666361808776855, 4.462616920471191, 4.53097677230835, 4.480189323425293, 5.413147449493408]}, {"title": "Development of a deep learning system for patient-specific real-time arrhythmia detection: Addressing choice of features and data imbalance", "description": "The Electrocardiogram (ECG) can be regarded as a prime tool in getting information on the cardiac functionality. The diagnostic power of an ECG is related to the ability to timely and accurately identify specific heartbeat alterations (arrhythmia). Several approaches to the automated analysis of ECG signals have been proposed so far, in the attempt to improve the quality of arrhythmia detection. The ECG signal, like many other biological signals, is generally non-linear, non-stationary, dynamic and complex: this hampers the extraction of characteristics to be used for any algorithmic analysis, with a severe impact on the efficiency of the overall diagnostic procedure. Specifically, the complexity of the ECG signal is related to the high variability of wave-form morphologies both for the same patient and across different patients, plus problems in the placements of electrodes on the patient\u2019s body. This work aims to increase the accuracy and speed of ECG diagnostic systems with respect to the real-time detection of arrhythmia events. A deep learning (DL) approach is proposed as a means to deal with the high variability in ECG signals. The implemented neural network has shown to be able to detect, starting from raw data, high-level ECG characteristics, with good generalization ability. The work is developed according to an \"intra-patient\" paradigm: the network is trained not only using generic samples, but also exploiting patient-specific data. The network consists of three convolutional layers, which are in charge of extracting features out of morphological information in ECG signals; subsequently, by considering also temporal features, the previous information is fed to a multi-layer-perceptron (MLP) to finalize the classification job. The performance evaluation of the developed classifier has been carried out taking into account different pre-processing strategies, best representation of morphological and temporal features, data augmentation techniques and fine-tuning strategies. The results over a classical, standard benchmark database (MIT-BIH DB), show that the network performance is comparable (and in some cases superior) to what it has been obtained in other works with the same paradigm. In accordance with the AAMI recommendations, the most challenging classes to identify are the Supraventricular Ectopic Beat (SVEB) and the Ventricular Ectopic Beat (VEB). For this reason, the performance on this class, based on Sensitivity (Sen) and Positive predicted value (Ppv), are representative of the effective robustness of the overall approach. The network reaches 60.21% and 79.85% for the SVEB class and 91.05% and 95.34% for the VEB class in terms of Sen and Ppv respectively. Such results show the competitive performance of this work with respect to the state-of-the-art. Finally, it is important to underline that the shallow depth of the network, as well as the limited number of steps to get to the final classification of input data, make the proposed system particularly well suited for real-time analysis in embedded devices", "position": {"x": -2.6344703554165894, "y": 2.202192536454997, "z": -1.8475233695722495}, "dissimilarity": [4.727444648742676, 4.574948310852051, 3.5145273208618164, 5.953194618225098, 5.631091594696045, 4.851315975189209, 4.446403980255127, 6.560975551605225, 5.217075824737549, 5.035093784332275, 4.628945350646973, 5.539573669433594, 4.510400772094727, 4.572083473205566, 3.638519525527954, 5.699166774749756, 5.18625020980835, 0.0, 4.871311187744141, 5.0532002449035645, 4.954205513000488, 5.051774501800537, 5.221534252166748, 5.870739459991455, 3.634476661682129, 6.299253940582275, 4.971749305725098, 5.096002578735352, 3.758017063140869, 6.817257881164551, 5.075732231140137, 5.038660526275635, 5.719197750091553, 4.867557525634766, 5.188806056976318, 5.565761566162109, 4.134316444396973]}, {"title": "Multi-task Deep Learning in the Software Development domain", "description": "Nowadays, almost every aspect of life depends on reliable high-quality software so there is a high demand for software tools that could help this development process. One direction of improvement consists on the use of Deep Learning techniques but we might face problems related to limited labeled datasets available, model overfitting that prevents the effective generalization, and energy consumption for the training process. In this thesis, we investigate how Multi-task Deep Learning can tackle these issues in the software development domain applying it to tasks that involve the manipulation of English and four programming languages namely Python, SQL, C#, and Java. We adapt the Transformer model architecture, actual state-of-the-art for sequence-to-sequence manipulation problems, to seven supervised tasks and the self-supervised language model and we explore whether we get benefits on the training of single tasks compared to multiple tasks together. We show the performance of our models and we compare our results with state-of-the-art counterparts that solved the tasks with the same datasets. We conclude that, given enough computing resources, Multi-task Deep Learning with the Transformer architecture is a promising framework to deal with software development domain tasks. To the best of our knowledge, this is the first work that applies large-scale multi-task models to software development tasks that involve source code and self-supervised and supervised tasks", "position": {"x": -1.1022048450424338, "y": 0.07403682710665324, "z": 2.2902401773722074}, "dissimilarity": [3.257836103439331, 3.4521231651306152, 3.4897797107696533, 5.43565034866333, 4.529058933258057, 3.1492857933044434, 4.81038236618042, 5.898297309875488, 4.205384731292725, 3.346632957458496, 3.2917978763580322, 3.8872456550598145, 3.4866013526916504, 3.6964986324310303, 5.000703811645508, 5.1186347007751465, 4.311297416687012, 4.871311187744141, 0.0, 3.021566152572632, 3.9107959270477295, 3.626476526260376, 4.758520603179932, 5.099777698516846, 3.841182231903076, 5.616039752960205, 4.003199100494385, 3.4325594902038574, 4.451973915100098, 6.354942321777344, 4.248501300811768, 4.462344646453857, 5.1199188232421875, 2.994185447692871, 4.0209059715271, 3.810810089111328, 4.382752418518066]}, {"title": "A quantitative study of mutation and fault tolerance of pyro inference programs", "description": "Probabilistic programming allows users to model complex probability distributions and perform inference on such models. Since probabilistic reasoning and inference is a foundational technology of statistical learning programs, adoption of probabilistic programming systems has been growing in the past few years. Pyro is a commonly used probabilistic programming system written in Python that is based on the PyTorch deep learning framework and has become very popular for machine learning applications. However, since Pyro uses a deep learning framework to sample from distributions, there is a need to evaluate the approximate nature of computations and the resilience of probabilistic programs. Additionally, there is also a significant need to systemically test probabilistic programs to identify major errors. In this thesis, we systemically evaluate, test, and analyze Pyro probabilistic inference functions and programs. Since mutation testing is a well-established approach to test software against fault injections, we apply mutations to Pyro inference functions by using MutPy, a mutation testing tool for Python programs. Specifically, we use three popular inference programs as our testing suite and conduct mutation injection experiments on the Pyro inference library. Next, we analyze the data corruption and the amount of error introduced by mutations on inference programs. We provide a collective study of mutation tolerance of Pyro inference functions and programs where we also analyze mutation operators of MutPy and high-order mutations.U of I Onlyundergraduate senior thesis not recommended for open acces", "position": {"x": -0.9443379306330767, "y": 1.0580781739708571, "z": 2.7994352093692756}, "dissimilarity": [3.58870530128479, 3.4365766048431396, 4.037565231323242, 5.3923659324646, 5.00469446182251, 2.7331831455230713, 4.888014793395996, 5.9703898429870605, 4.652491569519043, 3.5629234313964844, 3.4720394611358643, 3.6184537410736084, 3.7584569454193115, 3.9996163845062256, 5.247060775756836, 5.321616172790527, 4.7071661949157715, 5.0532002449035645, 3.021566152572632, 0.0, 3.813997268676758, 3.5717742443084717, 4.844304084777832, 5.443268299102783, 4.198505401611328, 5.961499214172363, 4.291802406311035, 4.081952095031738, 4.270513534545898, 6.39812707901001, 4.702739238739014, 4.787326335906982, 5.081246376037598, 2.501610517501831, 4.508153915405273, 4.345577716827393, 4.500080108642578]}, {"title": "Deep chain learning collusions over network with improved blockchain security", "description": "Smart contracts in blockchains can be executed for identifying and verifying images, text, signatures, information within forms and legal documents, etc., by collusions over network, benefitting various industry use cases. The identifications and verifications for digital identity currently follow a centralized architecture and may or may not include deep learning technologies with more secured data-centric authentications - a gap in the overall paradigm. This leads to bottlenecks with security issues involving many concurrent processes with many concurrent parties, resulting in longer service times and a lack of trust, thus impacting the overall Quality of Service (QoS). This thesis introduces a concept called \u201cDeep Chain Learning,\u201d which provides a secured technique for integrating deep learning within smart contracts in a blockchain in a decentralized architecture. The smart contracts trigger execution of neural network-based models for deep learning on component images, signatures, etc., by each or some of the parties at respective nodes in a blockchain. A user-authentication mechanism allows for accessing different object components by each party in the blockchain to execute deep learning. The inference results drawn from each of these parties are written to the blockchain, and shared across all parties. The implementation of \u201cDeep Chain Learning\u201d uses an example driver\u2019s license identification and verification process, as part of an auto insurance application. It also enables user access control privileges as a security measure for deep chain learning. Performance is evaluated for three use cases including auto insurance and healthcare applications. Results show that the distribution of inference tasks of component images among multiple parties lead to an almost linear reduction in cost, when compared to the control variable, which is a centralized, sequential mode of execution. The solution not only reduces the QoS, but with the security feature enabled, improves the overall trust in the paradigm.U of I Onlyundergraduate senior thesis not recommended for open acces", "position": {"x": 1.76526672417206, "y": -0.4633152583069465, "z": 2.215488986808264}, "dissimilarity": [3.568483829498291, 3.668550491333008, 4.354455471038818, 5.048092365264893, 4.84700870513916, 3.7133994102478027, 4.768709659576416, 5.491211891174316, 3.7690627574920654, 4.422765731811523, 3.440725326538086, 3.3964993953704834, 3.909933090209961, 4.093939781188965, 5.025208473205566, 5.581986904144287, 4.273778915405273, 4.954205513000488, 3.9107959270477295, 3.813997268676758, 0.0, 3.244986057281494, 5.0320515632629395, 5.162003517150879, 4.477968692779541, 5.5878424644470215, 3.3676419258117676, 4.142664909362793, 4.59240198135376, 6.140091896057129, 4.56030797958374, 4.649266719818115, 5.287056922912598, 3.883190870285034, 4.707184791564941, 4.3025336265563965, 4.985752582550049]}, {"title": "DIMA system for real-time object detention", "description": "In recent years, breakthroughs in machine learning and deep learning have shown their unlimited potential in autonomous driving, unmanned stores, etc. However, their superior capabilities come with high computational costs. While these techniques can achieve reasonable performance on servers or workstations equipped with multiple GPUs, they cannot be easily deployed on edge or IoT platforms. This challenge demands a computationally efficient solution to enable machine learning or deep learning for the edge. Towards this goal, this research focused on developing a system to accelerate video inference by utilizing deep in memory architecture (DIMA) ICs designed and prototyped recently in our research group. The DIMA IC embeds mixed-signal compute blocks within SRAM memory array to accelerate machine learning models for image classification with 3.1x higher power efficiency and 2.1x lower inference latency. While DIMA IC achieved fast inference on single cropped images, its overall test setup was not optimized for video inference. To address this issue, we replaced the original MCU + PC in the previous setup with a Raspberry Pi and enabled it to directly process image information and control the chip. As a result, the system performance improved from more than 10 seconds per image to around 13 ms inference time. This also enabled us to complete the system with real camera input. With frames streaming into the Raspberry Pi, it will preprocess the image and regional proposal for the chip. The chip will accelerate the image classification process and provide the system with real-time object recognition capability.U of I Onlyundergraduate senior thesis not recommended for general acces", "position": {"x": 1.4653145112731365, "y": 1.452916005713809, "z": 1.8025018632848993}, "dissimilarity": [3.3595471382141113, 3.271277666091919, 4.472334861755371, 4.982318878173828, 4.7919464111328125, 3.6498076915740967, 4.726858139038086, 5.64859676361084, 3.7394769191741943, 4.010089874267578, 3.425191640853882, 2.9507863521575928, 3.4762120246887207, 3.717194080352783, 4.746350288391113, 5.376788139343262, 3.6901028156280518, 5.051774501800537, 3.626476526260376, 3.5717742443084717, 3.244986057281494, 0.0, 5.080806255340576, 5.075524806976318, 3.910196304321289, 5.686926364898682, 2.925048351287842, 3.6720993518829346, 4.240351676940918, 6.11187219619751, 4.937863349914551, 4.811128616333008, 4.938235759735107, 3.7122888565063477, 4.808548927307129, 4.011453151702881, 4.954789161682129]}, {"title": "Activity Monitoring of Islamic Prayer (Salat) Postures using Deep Learning", "description": "In the Muslim community, the prayer (i.e. Salat) is the second pillar of Islam, and it is the most essential and fundamental worshiping activity that believers have to perform five times a day. From a gestures' perspective, there are predefined human postures that must be performed in a precise manner. However, for several people, these postures are not correctly performed, due to being new to Salat or even having learned prayers in an incorrect manner. Furthermore, the time spent in each posture has to be balanced. To address these issues, we propose to develop an artificial intelligence assistive framework that guides worshippers to evaluate the correctness of the postures of their prayers. This paper represents the first step to achieve this objective and addresses the problem of the recognition of the basic gestures of Islamic prayer using Convolutional Neural Networks (CNN). The contribution of this paper lies in building a dataset for the basic Salat positions, and train a YOLOv3 neural network for the recognition of the gestures. Experimental results demonstrate that the mean average precision attains 85% for a training dataset of 764 images of the different postures. To the best of our knowledge, this is the first work that addresses human activity recognition of Salat using deep learning.info:eu-repo/semantics/publishedVersio", "position": {"x": 0.9375120738732294, "y": 0.22523983056437977, "z": -3.385309087661619}, "dissimilarity": [4.551424026489258, 4.7275567054748535, 4.205778121948242, 5.52103328704834, 5.025503158569336, 5.031515121459961, 4.745466709136963, 5.963654518127441, 5.146759986877441, 4.775719165802002, 4.693143367767334, 5.289665222167969, 4.993160724639893, 4.609792709350586, 5.374106407165527, 5.468994617462158, 5.54579496383667, 5.221534252166748, 4.758520603179932, 4.844304084777832, 5.0320515632629395, 5.080806255340576, 0.0, 5.690113544464111, 4.549352169036865, 5.5218424797058105, 5.272138595581055, 5.031188488006592, 5.267331600189209, 6.1099419593811035, 4.5424065589904785, 4.8548479080200195, 3.1643383502960205, 4.884132385253906, 5.1816253662109375, 5.190946578979492, 5.011936664581299]}, {"title": "Smart Ice control system based on Machine and Deep Learning Techniques", "description": "La tesi sperimentale \u00e8 stata svolta presso il Centro di ricerca di microgravit\u00e0 presso l'Universit\u00e9 Libre de Bruxelles. L'obiettivo principale del progetto era quello di creare uno Smart System per il rilevamento e il controllo del ghiaccio. Lo Smart System \u00e8 stato creato per prevedere la formazione del ghiaccio applicando le tecniche di machine learning e di deep learning al fine di sciogliere il ghiaccio sulla superficie dell'ala se questo \u00e8 formato. Sono stati usati diversi approcci per avere la migliore affinit\u00e0 per la previsione della formazione di ghiaccio su una superficie alare senza alcun intervento umano. Il sistema \u00e8 completamente autonomo e l'intelligenza artificiale rende il sistema anche in grado di apprendere dall'addestramento precedente e migliorare le proprie decisioni in merito alla formazione del ghiaccio. Le previsioni vengono eseguite applicando due metodi: uno chiamato passivo per quanto riguarda il fenomeno naturale chiamato \"sopraffusione\" quando c'\u00e8 una diminuzione naturale della temperatura; il secondo \u00e8 un metodo attivo utilizzato quando la temperatura scende sotto -15 \u00b0 C e viene inviato un impulso per prevedere se si forma o meno del ghiaccio sull'ala. In particolare per la parte dello scioglimento del ghiaccio \u00e8 stata utilizzata una resistenza creata con strisce di Grafene. Questo progetto \u00e8 in collaborazione con Airbus. The experimental thesis has been carried out at Microgravity Research Center at Universit\u00e9 Libre de Bruxelles. The main goal of the project was to create a Smart System for ice detection and control. The Smart System has been created to make prediction of the ice formation applying machine learning and deep learning techniques in order to melt the ice over the wing surface if it is formed. Several approaches have been used in order to have the best affinity for prediction of ice formation over a wing surface without any human intervention. The system is completely autonomous and artificial intelligence makes the system able also to learn from the previous training and improve its own decisions regarding the ice formation. The predictions are performed applying two methods: one called passive regarding the natural phenomenon called \u201csupercooling\u201d when there is a natural temperature decreasing; the second one is an active method used when the temperature decreases under -15\u00b0C , and a pulse is sent to predict if ice is formed or not over the wing. In particular for the part regarding the Ice melting has been created a resistance in Graphene. This project is in collaboration with Airbus", "position": {"x": 2.1846608837849346, "y": -3.008197405441274, "z": 0.050158659832535514}, "dissimilarity": [4.72881555557251, 4.926558494567871, 4.5597639083862305, 3.7925686836242676, 4.229608535766602, 5.6485466957092285, 5.272087097167969, 3.944782018661499, 5.057838439941406, 4.912823677062988, 4.625190734863281, 5.323109149932861, 5.278724193572998, 5.013948440551758, 5.2130327224731445, 6.083600044250488, 5.118865013122559, 5.870739459991455, 5.099777698516846, 5.443268299102783, 5.162003517150879, 5.075524806976318, 5.690113544464111, 0.0, 5.462396144866943, 4.056807041168213, 5.03683614730835, 5.248789310455322, 5.580228805541992, 4.201895713806152, 4.9384236335754395, 5.508326053619385, 5.749621391296387, 5.675194263458252, 3.508091449737549, 4.54047966003418, 5.359873294830322]}, {"title": "Automated sheep facial expression classification using deep transfer learning", "description": "Digital image recognition has been used in the different aspects of life, mostly in object classification and detections. Monitoring of animal life with image recognition in natural habitats is essential for animal health and production. Currently, Sheep Pain Facial Expression Scale (SPFES) has become the focus of monitoring sheep from facial expression. In contrast, pain level estimation from facial expression is an efficient and reliable mark of animal life. However, the manual assessment is lack of accuracy, time-consuming, and monotonous. Hence, the recent advancement of deep learning in computer vision helps to classify facial expression as fast and accurate. In this paper, we proposed a sheep face dataset and framework that uses transfer learning with fine-tuning for automating the classification of normal (no pain) and abnormal (pain) sheep face images. Current state-of-the-art convolutional neural networks (CNN) based architectures are used to train the sheep face dataset. The data augmentation, L2 regularization, and fine-tuning has been used to prepare the models. The experimental results related to the sheep facial expression dataset achieved 100% training, 99.69% validation, and 100% testing accuracy using the VGG16 model. While employing other pre-trained models, we gained 93.10% to 98.4% accuracy. Thus, it shows that our proposed model is optimal for high-precision classification of normal and abnormal sheep faces and can check on a comprehensive dataset. It can also be used to assist other animal life with high accuracy, save time and expenses.info:eu-repo/semantics/publishedVersio", "position": {"x": -0.6532754127478387, "y": 2.645723557201836, "z": -1.0131575578942207}, "dissimilarity": [3.8389158248901367, 3.6118617057800293, 3.504824161529541, 5.525007247924805, 5.139072895050049, 4.145291805267334, 4.33198881149292, 6.175036430358887, 4.758658409118652, 4.160906791687012, 3.828252077102661, 4.7408599853515625, 3.8246943950653076, 3.3783481121063232, 3.954335927963257, 4.732461452484131, 4.470975399017334, 3.634476661682129, 3.841182231903076, 4.198505401611328, 4.477968692779541, 3.910196304321289, 4.549352169036865, 5.462396144866943, 0.0, 6.046774387359619, 4.292212963104248, 4.143571376800537, 3.5072813034057617, 6.500308513641357, 4.782037258148193, 4.636678218841553, 4.680454730987549, 4.020641326904297, 4.795183181762695, 4.809972286224365, 4.345520496368408]}, {"title": "Old Meets New: Media in Education \u2013 Proceedings of the 61st International Council for Educational Media and the XIII International Symposium on Computers in Education (ICEM&SIIE'2011) Joint Conference", "description": "A confer\u00eancia ICEM&SIIE'2011 foi organizada pela Universidade de Aveiro (Portugal) \u2013 membro do European Consortium of Innovative Universities \u2013 e pretendeu reunir investigadores, professores e outros profissionais, a n\u00edvel nacional e internacional, em torno de um tema aglutinador que pretendeu despoletar e colocar a t\u00f3nica da discuss\u00e3o na dualidade \u2015old/new\u2016, ou seja, os participantes foram convidados a discutir: - os media na educa\u00e7\u00e3o em ambas as perspetivas, mais tradicionais ou modernas, com incid\u00eancia numas ou noutras ou, ainda, numa perspetiva comparativa; - a conjuga\u00e7\u00e3o, adapta\u00e7\u00e3o e ado\u00e7\u00e3o dos media consoante os contextos e objetivos de utiliza\u00e7\u00e3o; - o que os media implicam em termos de tecnologia, barreiras profissionais e /ou sociais; - a rela\u00e7\u00e3o custo-benef\u00edcio da utiliza\u00e7\u00e3o dos media em contexto de aprendizagem; - os media em fun\u00e7\u00e3o dos diversos contextos educativos e dos perfis de aprendizagem dos alunos. Para a confer\u00eancia foram selecionados 76 artigos organizados em 15 sess\u00f5es paralelas, 13 posters e 9 workshops. A confer\u00eancia caracterizou-se pelo car\u00e1ter internacional dos contributos, reunindo 38 artigos em portugu\u00eas, 32 em l\u00edngua inglesa e 6 em espanhol. Estas atas encontram-se organizadas de acordo com o programa da confer\u00eancia. Em primeiro lugar incluem-se os artigos (full paper e short paper) por sess\u00e3o, seguem-se os posters e, finalmente, o resumo relativo aos workshops.The ICEM&SIIE'2011 conference was organised by the University of Aveiro (Portugal) \u2013 a member of the European Consortium of Innovative Universities \u2013 and aimed at gathering researchers, teachers and other professionals, at national and international level, around a focal topic that might trigger and centre the discussion on the \u2015old/new\u2016 duality of media in education. Participants were invited to discuss: - old and new media in education, in isolation or comparatively; - how old and new media in education can be combined, adopted and adapted; - what old and new media in education imply in terms of technological, professional and social barriers; - what cost-benefit relationships old and new media in education entail; - how to compare old and new media in education given their particular educational contexts and the students' learning profiles. 76 papers were selected and organised in 15 paralel sessions, 13 posters and 9 workshops. The conference is characterized by the international character of contributions, gathering 38 papers in Portuguese, 32 in English and 6 in Spanish. These procedings are organised according to the programme of the conference. First we find the full and short papers, per session, then posters and finally the abstracts for the workshops", "position": {"x": -0.41850834101332485, "y": -4.331473095484618, "z": -0.29204961938348556}, "dissimilarity": [5.345144748687744, 5.44033670425415, 5.099156379699707, 4.143187046051025, 3.2826762199401855, 6.194562911987305, 5.563389301300049, 4.336590766906738, 5.2574567794799805, 5.120466709136963, 5.366824150085449, 5.838648796081543, 5.789344787597656, 5.660245895385742, 5.915543556213379, 6.233067989349365, 5.9459686279296875, 6.299253940582275, 5.616039752960205, 5.961499214172363, 5.5878424644470215, 5.686926364898682, 5.5218424797058105, 4.056807041168213, 6.046774387359619, 0.0, 5.632076740264893, 5.679502964019775, 6.0694780349731445, 4.526548385620117, 4.923110485076904, 5.275130271911621, 5.66665506362915, 6.043262004852295, 4.511174201965332, 5.1594157218933105, 5.6230268478393555]}, {"title": "Analysis and Design of a DNN for Smoke/Fire Video Feature Extraction and its Integration in Low-cost and Low-power Embedded Systems", "description": "Design and analysis of a DNN architecture for smart fire and smoke detection in indoor/outdoor environment. The algorithm uses a two-subnetwork architecture joined with an alarm-triggering code, it was trained on custom dataset and tested versus most recent state of the art solutions and traditional AI-based methods through the computation of performance indices such as Accuracy, Precison, Recal and F1-score. The algorithm deployment on low-end embedded systmes, like Raspberry Pi 3 and Nvidia Jetson Nano, is then discussed in detail confronting the resulting performance against both deep-Learning-based and non-DL-based solutions from the State of the Art and traditionally employed techniques such as fine-tuned VGG DNN. Results show that the proposed solution outperform benchmarks in term of performance indices while achieving computational cost comparable to non-DL-based solutions", "position": {"x": 2.4535494899210204, "y": 0.5961773293955516, "z": 1.3209430807766451}, "dissimilarity": [3.674208164215088, 3.431434154510498, 4.6400628089904785, 5.088454246520996, 4.769284248352051, 3.8319358825683594, 4.823392868041992, 5.724910736083984, 3.4958655834198, 4.304684162139893, 3.202246904373169, 3.398285150527954, 3.8647656440734863, 3.808417797088623, 4.726866722106934, 5.81631326675415, 2.7130956649780273, 4.971749305725098, 4.003199100494385, 4.291802406311035, 3.3676419258117676, 2.925048351287842, 5.272138595581055, 5.03683614730835, 4.292212963104248, 5.632076740264893, 0.0, 4.110659599304199, 4.629873752593994, 6.167028427124023, 4.909412860870361, 4.825894355773926, 5.25496768951416, 4.153341770172119, 4.5870361328125, 4.156734466552734, 5.142883777618408]}, {"title": "Algorithm and Architecture trade-offs for Digital Signal Processing design: case studies in Space and Assistive Technology applications", "description": "Nowadays, the conflicting necessities to exploit emerging technologies, such as Deep Learning, and to process data on the edge embedded platforms, featuring limited computing resources, has exacerbated the complexity of design trade-offs in Digital Signal Processing (DSP) design. Moreover, additional constraints are set by the peculiarities of niche market applications, whose reduced sales volumes and intrinsic requirements often prevent R&D prototypes from improving their technology maturity. Because of that, in this work, we investigate the design of DSP-based systems with respect to several case studies in the field of space and assistive technology. In the field of assistive technology, our research questions involve the implementation of a portable Voice User Interface (VUI) for Italian speakers with severe dysarthria and the design of a low-effort Human-Machine Interface to control a power wheelchair-mounted manipulator to assist people with upper-limb disabilities. In the field of space applications, we faced the problem of the limited downlink throughput of Earth Observation (EO) satellites through the deployment of Deep Neural Networks (DNNs) on board space-craft to compress their downlink data. As a side project, we showed a flexible implementation of the CCSDS 131-2-B-1 standard, a payload telemetry transmitter ensuring high downlink throughput thanks to the use of adaptive coding and modulation", "position": {"x": 0.714454010796148, "y": 2.7606034190814084, "z": 0.8891348427172384}, "dissimilarity": [3.14943265914917, 3.8795783519744873, 3.9049673080444336, 5.21055793762207, 4.603420734405518, 3.6200246810913086, 4.99324893951416, 5.640610218048096, 4.1098103523254395, 3.847090482711792, 4.160487651824951, 4.059154987335205, 3.6444952487945557, 3.332350254058838, 5.021250247955322, 4.970805644989014, 4.300152778625488, 5.096002578735352, 3.4325594902038574, 4.081952095031738, 4.142664909362793, 3.6720993518829346, 5.031188488006592, 5.248789310455322, 4.143571376800537, 5.679502964019775, 4.110659599304199, 0.0, 4.448699474334717, 6.162572860717773, 4.846792221069336, 4.935283184051514, 4.868387699127197, 3.82114839553833, 4.450876712799072, 4.331315040588379, 4.676107883453369]}, {"title": "Prostate cancer diagnosis by deep learning", "description": "Prostate cancer diagnosis by biopsy images of human tissue requires experienced trained pathologists and the cost is high. To facilitate prostate cancer diagnosis, we built and trained binary classifiers using deep convolutional neural networks (CNNs) on two datasets: one contains cancerous and healthy biopsy images of prostate tissues (referred as Dataset1), and the other contains biopsy images of tissues with recurred cancer and fully recovered tissues (referred as Dataset2). We extracted patches from biopsy images of human tissues, then built and trained CNN models to classify the patches. We achieved 82% test accuracy on Dataset1 and 63% accuracy on Dataset2. In addition, we used ensemble methods to further boost the performance. With predictions of all patches in our datasets, we performed majority voting on the image level, and the accuracy increases by 5% to 10% on the first dataset. Then we used Bootstrap Aggregation (Bagging) to further increase accuracy to 100% on Dataset1. However, the two-step ensemble methods above have little influence on the accuracy of Dataset2. When visualizing the predictions on the second dataset returned by our models, no clear patterns are found that can distinguish the two classes.U of I Onlyundergraduate senior thesis not recommended for open acces", "position": {"x": -2.4742048617587042, "y": 2.200105186897914, "z": -0.39898858286053096}, "dissimilarity": [4.310851573944092, 4.135000228881836, 3.840690851211548, 5.595180511474609, 5.399021625518799, 4.3480544090271, 3.6681747436523438, 6.151061534881592, 5.26132869720459, 3.9632084369659424, 4.156991004943848, 5.027292251586914, 3.9806840419769287, 4.105634689331055, 3.5800249576568604, 5.4678826332092285, 5.145490646362305, 3.758017063140869, 4.451973915100098, 4.270513534545898, 4.59240198135376, 4.240351676940918, 5.267331600189209, 5.580228805541992, 3.5072813034057617, 6.0694780349731445, 4.629873752593994, 4.448699474334717, 0.0, 6.516510963439941, 5.172796726226807, 4.767767906188965, 5.007562637329102, 3.944817543029785, 4.989149570465088, 4.77897310256958, 3.766042947769165]}, {"title": "Analisi della voce attraverso le Reti Neurali: applicazione di software nello screening delle disfonie.", "description": "Background. La disfonia \u00e8 un sintomo di alterazione qualitativa e/o quantitativa dell\u2019emissione vocale che accompagna una modificazione strutturale o funzionale di uno o pi\u00f9 organi coinvolti nella produzione della voce. Per alcune categorie di lavoratori, l\u2019argomento relativo ai disturbi professionali derivanti da uno scorretto utilizzo della voce non \u00e8 ancora stato definito pienamente in termini di screening e prevenzione. Lo scopo del nostro studio \u00e8 quello di fornire file relativi alla valutazione della voce da analizzare tramite i Deep Learning, algoritmi di apprendimento automatico ispirati alle reti neurali biologiche con promettenti applicazioni nell\u2019ambito di varie specialit\u00e0 mediche. L\u2019obiettivo \u00e8 quello di proporre uno strumento di screening di primo livello della voce in grado di indirizzare i soggetti disfonici dal foniatra o dal logopedista. Sar\u00e0 poi loro compito eseguire su tali pazienti un\u2019analisi completa della voce, al fine di stabilire la causa della disfonia ed impostare un\u2019adeguata terapia. Materiali e Metodi. Sono stati utilizzati 828 file audio raccolti tra Gennaio 2013 e Luglio 2019 e convertiti in spettrogrammi. Ciascuno spettrogramma \u00e8 stato catalogato secondo una nuova classificazione da noi ideata per poter essere poi analizzato dalle reti neurali. Risultati e conclusione. La nostra classificazione ha fornito risultati altamente accurati sia in termini di sensibilit\u00e0 che di specificit\u00e0 rispetto al gold standard per la diagnosi di disfonia rappresentato dalla laringostroboscopia. Inoltre, la successiva analisi degli spettrogrammi con le reti neurali ha permesso di ottenere risultati promettenti, raggiungendo percentuali di riconoscimento dei soggetti sani e patologici nel 94% dei casi", "position": {"x": 0.7154629606273922, "y": -3.875214423216406, "z": -2.9605934196255985}, "dissimilarity": [5.9832892417907715, 6.055348873138428, 5.464784622192383, 2.895159959793091, 4.771729946136475, 6.562702655792236, 6.052890777587891, 2.6285831928253174, 6.005805492401123, 5.994634628295898, 6.135010242462158, 6.150172233581543, 6.211008548736572, 6.3176140785217285, 6.408110618591309, 6.563302993774414, 6.58211612701416, 6.817257881164551, 6.354942321777344, 6.39812707901001, 6.140091896057129, 6.11187219619751, 6.1099419593811035, 4.201895713806152, 6.500308513641357, 4.526548385620117, 6.167028427124023, 6.162572860717773, 6.516510963439941, 0.0, 5.536313056945801, 5.935194492340088, 6.282663345336914, 6.610803127288818, 4.978758811950684, 5.96782112121582, 6.213193416595459]}, {"title": "Sympathetic Rationality in Elementary Schools", "description": "262 p.Thesis (Educat.D.)--University of Illinois at Urbana-Champaign, 1980.Sympathetic Rationality in Elementary Schools is a study concerned with cultivating deep learning experiences interlaced with feeling. It recognizes the sensitive relationship between knowing and emotions. The concept was devised by Robert Jerome Starratt in a dissertation entitled, The Individual and the Educated Imagination: An Essay in Curriculum Theory, which was accepted by the Graduate College of the University of Illinois in 1969. Building on the Starratt concepts which were geared for a secondary school curriculum, this study attempts to identify the factors influencing elementary school climates which promote the learning experiences described in the first sentence. Two elementary schools were studied which might be placed at opposite ends of a continuum ranging economically from poor to affluent. The study is concerned with multiple forces affecting the schools over the ten year period, from the filing of the Starratt dissertation to the completion of the present study. The study focused on such broad influences as declining enrollments, desegregation and political forces which are changing the course of education in the United States.The study advances the premise that curriculum theory and program changes are not likely to succeed unless the underlying forces affecting the educational climate are understood and controlled. The study also suggests that one of the best hopes for achieving the kinds of knowing interlaced with feeling is through improvements in the teacher education process. The value of university laboratory schools in this process is carefully explored. Finally, the delicate relationship between available resources, number of positions, change and the conservation of the school's heritage is a primary concern. The study concludes with the presentation of an administrative strategy or guidelines designed to produce the desired learnings.U of I OnlyRestricted to the U of I community idenfinitely during batch ingest of legacy ETD", "position": {"x": -2.4786901026392814, "y": -1.989875588358903, "z": -0.8812218051026858}, "dissimilarity": [4.288661003112793, 4.380533695220947, 3.8064639568328857, 5.0667009353637695, 4.396870136260986, 4.549394130706787, 4.660665512084961, 5.411446571350098, 4.831540107727051, 4.467881202697754, 4.132500648498535, 5.004019260406494, 4.662146091461182, 4.937013149261475, 5.124495506286621, 5.577978610992432, 5.511565208435059, 5.075732231140137, 4.248501300811768, 4.702739238739014, 4.56030797958374, 4.937863349914551, 4.5424065589904785, 4.9384236335754395, 4.782037258148193, 4.923110485076904, 4.909412860870361, 4.846792221069336, 5.172796726226807, 5.536313056945801, 0.0, 3.7005300521850586, 5.552088737487793, 4.6724653244018555, 4.419400215148926, 5.070355415344238, 4.781495094299316]}, {"title": "Students' Learning Approaches and Their Understanding of Some Chemical Concepts in Eighth-Grade Science", "description": "303 p.Thesis (Ph.D.)--University of Illinois at Urbana-Champaign, 1997.Analysis of the target students' discourse and actions during the group activities and their interview responses revealed differences between the deep and surface learning approaches regarding generative thinking, nature of explanations, asking questions, metacognitive activity, and approach to tasks. Strategies associated with a deep learning approach included generating mental images and analogies, hypothesizing, thought experimenting and predicting possible outcomes, self-explaining and theorizing, invoking personal experiences and prior knowledge and applying them to new situations, asking questions, thinking of specific examples, and looking for coherence by seeking patterns and relating different aspects of the task. When students used a deep approach, they also constantly monitored and self-evaluated the status of their comprehension, self-questioned, self-corrected their errors, attended to contradictory information, considered limitations in their own or others' ideas and critiqued them.U of I OnlyRestricted to the U of I community idenfinitely during batch ingest of legacy ETD", "position": {"x": -3.0635696835806407, "y": -1.6643120102047, "z": 0.9217552933748294}, "dissimilarity": [4.320152759552002, 4.128567695617676, 4.020384311676025, 5.143200874328613, 4.8603105545043945, 4.785196781158447, 4.9445061683654785, 5.7675299644470215, 5.331027030944824, 4.270042419433594, 4.067930698394775, 5.164353370666504, 4.223714828491211, 4.9839630126953125, 5.137357711791992, 5.929627895355225, 5.410195350646973, 5.038660526275635, 4.462344646453857, 4.787326335906982, 4.649266719818115, 4.811128616333008, 4.8548479080200195, 5.508326053619385, 4.636678218841553, 5.275130271911621, 4.825894355773926, 4.935283184051514, 4.767767906188965, 5.935194492340088, 3.7005300521850586, 0.0, 5.5448102951049805, 4.682579517364502, 4.749293804168701, 4.527869701385498, 5.135485649108887]}, {"title": "AI-based Pilgrim Detection using Convolutional Neural Networks", "description": "Pilgrimage represents the most important Islamic religious gathering in the world where millions of pilgrims visit the holy places of Makkah and Madinah to perform their rituals. The safety and security of pilgrims is the highest priority for the authorities. In Makkah, 5000 cameras are spread around the holy mosques for monitoring pilgrims, but it is almost impossible to track all events by humans considering the huge number of images collected every second. To address this issue, we propose to use an artificial intelligence technique based on deep learning and convolutional neural networks to detect and identify Pilgrims and their features. For this purpose, we built a comprehensive dataset for the detection of pilgrims and their genders. Then, we develop two convolutional neural networks based on YOLOv3 and Faster-RCNN for the detection of Pilgrims. Experiment results show that Faster RCNN with Inception v2 feature extractor provides the best mean average precision over all classes (51%). A video demonstration that illustrates a real-time pilgrim detection using our proposed model is available at [1].info:eu-repo/semantics/publishedVersio", "position": {"x": 2.383446405944592, "y": 0.9420782216742667, "z": -2.8774939552561056}, "dissimilarity": [4.826199531555176, 4.772590160369873, 4.755273342132568, 5.57895040512085, 5.1127424240112305, 5.391983985900879, 4.881715297698975, 6.073052406311035, 5.249598026275635, 4.70551061630249, 5.125859260559082, 5.400923252105713, 5.195328712463379, 4.399770736694336, 5.574424743652344, 4.899244785308838, 5.4666361808776855, 5.719197750091553, 5.1199188232421875, 5.081246376037598, 5.287056922912598, 4.938235759735107, 3.1643383502960205, 5.749621391296387, 4.680454730987549, 5.66665506362915, 5.25496768951416, 4.868387699127197, 5.007562637329102, 6.282663345336914, 5.552088737487793, 5.5448102951049805, 0.0, 5.189273357391357, 5.489660263061523, 5.039111137390137, 5.200784206390381]}, {"title": "Automated theorem proving agent with memory", "description": "Proof assistants are interactive software tools that performs automatic proof checking. Training deep learning agents that are capable of interacting with proof assistants provide a novel way of developing automated theorem proving (ATP) systems. We observe that existing methods in this field, either supervised learning methods or RL, are all based on locality assumptions\u2013the agent generates proof tactics conditioning only on the information in the most recent time step, discarding any information in previous time steps. However, the correctness of such assumptions has never been verified. We propose a method that introduces global information in training deep-learning based ATP models. Our contributions can be summarized as follows: (a) we construct a LSTM-like memory mechanism across time steps for deep learning-based ATP agents, and (b) we build a pipeline for end-to-end training by constructing a differentiable estimation of the proof assistant, which can be viewed as a discrete function. We show the effectiveness of our method by building on the state-of-the-art ATP model, ASTatics.U of I OnlyUndergraduate senior thesis not recommended for open acces", "position": {"x": -1.8664404083850017, "y": 1.346394660817735, "z": 2.061514798145991}, "dissimilarity": [3.5978641510009766, 3.60066294670105, 3.9850220680236816, 5.623946666717529, 5.0475754737854, 2.6502113342285156, 4.95728063583374, 6.105893135070801, 4.382742881774902, 3.397017002105713, 3.3912997245788574, 4.184877395629883, 3.5192716121673584, 3.8507657051086426, 5.100447654724121, 5.598966598510742, 4.462616920471191, 4.867557525634766, 2.994185447692871, 2.501610517501831, 3.883190870285034, 3.7122888565063477, 4.884132385253906, 5.675194263458252, 4.020641326904297, 6.043262004852295, 4.153341770172119, 3.82114839553833, 3.944817543029785, 6.610803127288818, 4.6724653244018555, 4.682579517364502, 5.189273357391357, 0.0, 4.4696431159973145, 4.603677272796631, 4.260496139526367]}, {"title": "Generative Models of Human Mobility based on Deep Learning", "description": "Goal of the thesis is the generation of synthetic human mobility based on Deep Learning. Three different generative recurrent models have been implemented: a Seq2Seq Variational Autoencoder (VAE), a Generative Adversarial Network (GAN) and a Wasserstein GAN. The aim of this study is the generation of a synthetic dataset of GPS trajectories having characteristics and typical measures proper of the real human mobility. Scopo della tesi \u00e8 la generazione di mobilit\u00e0 umana sintetica basata suDeep Learning. Sono stati implementati tre modelli generativi: un Seq2Seq Variational Autoencoder (VAE), una Generative Adversarial Network (GAN) e una Wasserstein GAN. Obiettivo finale dello studio \u00e8 lagenerazione di un dataset sintetico di traiettorie GPS, avente caratteristiche e misure proprie della mobilit\u00e0 umana", "position": {"x": -0.6550896450270512, "y": -2.2025066481092916, "z": -0.18964558671818788}, "dissimilarity": [3.749558448791504, 3.7910945415496826, 3.4604220390319824, 4.075807571411133, 3.859713315963745, 4.277914047241211, 4.958622455596924, 4.4727630615234375, 4.61323356628418, 3.7143914699554443, 3.6512253284454346, 4.900423049926758, 4.251342296600342, 4.127871036529541, 5.023474216461182, 6.010293960571289, 4.53097677230835, 5.188806056976318, 4.0209059715271, 4.508153915405273, 4.707184791564941, 4.808548927307129, 5.1816253662109375, 3.508091449737549, 4.795183181762695, 4.511174201965332, 4.5870361328125, 4.450876712799072, 4.989149570465088, 4.978758811950684, 4.419400215148926, 4.749293804168701, 5.489660263061523, 4.4696431159973145, 0.0, 4.439333915710449, 4.250740051269531]}, {"title": "Machine Learning and AI for the Professional Translation Market", "description": "The report contains the summary of my work at Translated Srl. I worked for 6 months with the internship contract. During my internship, I mainly worked on two projects. The first project was where I spent my first four months of the internship. This time involved working for ModernMT speedups. We spent this time gathering existing resources on ways to speedup a NMT model. We filtered the ones suited better for ModernMT and we experimented the approaches on it. We were looking for approaches we can apply on ModernMT, which also works on the GPUs that we have in production. I spent the rest two months working on a future product MateDub which is under development currently. MateDub aims at performing automatic video dubbing using Artificial Intelligence. In both of these projects, I worked on primarily on AWS instances. Most of my work was carried on on-demand and spot instances of g4dn and p3 with Tesla and Volta GPU architectures respectively. Finally, work was tried on production GPUs of the company (production GPUs). The Deep Learning tasks were done in Python mainly using PyTorch libraries. The GPU profiling/debugging and programming tasks were performed using NVIDIA Nsight, NVIDIA Compute, NVTX-python and NVIDIA TensorRT. The data related tasks were performed using Python and Audacity. The presen- tation and reporting was done using Overleaf, Google Docs, Apple Keynotes and Tableau Desktop", "position": {"x": 0.14982121892895692, "y": -1.828790796678274, "z": 2.795341940817113}, "dissimilarity": [4.219302654266357, 3.9704675674438477, 4.461506366729736, 5.222973823547363, 4.649730682373047, 4.790546417236328, 5.1520304679870605, 5.563938140869141, 4.747995376586914, 3.7824809551239014, 3.9704084396362305, 4.167258262634277, 4.114663600921631, 4.412691593170166, 4.935129642486572, 5.7856645584106445, 4.480189323425293, 5.565761566162109, 3.810810089111328, 4.345577716827393, 4.3025336265563965, 4.011453151702881, 5.190946578979492, 4.54047966003418, 4.809972286224365, 5.1594157218933105, 4.156734466552734, 4.331315040588379, 4.77897310256958, 5.96782112121582, 5.070355415344238, 4.527869701385498, 5.039111137390137, 4.603677272796631, 4.439333915710449, 0.0, 5.246288299560547]}, {"title": "Deep learning for cardiologist-level myocardial infarction detection in electrocadiograms", "description": "Heart disease is the leading cause of death worldwide. Among patients with cardiovascular diseases, myocardial infarction is the main cause of death. In order to provide adequate healthcare support to patients who may experience this clinical event, it is essential to gather supportive evidence in a timely manner to help secure a correct diagnosis. In this thesis we design domain-inspired neural network models, trained, tested and validated with the Physikalisch-Technische Bundesanstalt (PTB) data set, to conduct a series of studies. First, acknowledging that the identification of suggestive electrocardiographic (ECG) changes may help in the classification of heart conditions, we adapt the ConvNetQuake neural network model---originally designed to identify earthquakes---to train, validate and test neural network models that take in from one to several ECG leads. This systematic analysis, first of its kind in the literature, indicates that out of 15 ECG leads, data from the v6, vz, and ii leads are critical to correctly identify myocardial infarction. Second, we show that using two independent train-validation-test data splits, namely, record-wise and patient-wise, does not change the finding that the combination of the leads v6, vz, and ii provides the best classification results for myocardial infarction, achieving 99.43% classification accuracy on a record-wise split, and 97.83% classification accuracy on a patient-wise split. These two results represent cardiologist-level performance for myocardial infarction detection after feeding only 10 seconds of raw ECG data into our multi-ECG-channel (v6-vz-ii) neural network model. Third, we show that our multi-ECG-channel neural network model achieves cardiologist-level performance without the need of any kind of manual feature extraction or data pre-processing.U of I OnlyUndergraduate senior thesis not recommended for open acces", "position": {"x": -3.2953400107340403, "y": 0.1977297545745633, "z": -0.8642568397490543}, "dissimilarity": [4.307633399963379, 4.546658515930176, 3.6839065551757812, 5.341111183166504, 4.871465682983398, 4.503316879272461, 4.335175037384033, 5.821974754333496, 5.054264545440674, 4.278541564941406, 4.493851184844971, 5.263599872589111, 4.342266082763672, 4.271213531494141, 4.240714073181152, 5.657176494598389, 5.413147449493408, 4.134316444396973, 4.382752418518066, 4.500080108642578, 4.985752582550049, 4.954789161682129, 5.011936664581299, 5.359873294830322, 4.345520496368408, 5.6230268478393555, 5.142883777618408, 4.676107883453369, 3.766042947769165, 6.213193416595459, 4.781495094299316, 5.135485649108887, 5.200784206390381, 4.260496139526367, 4.250740051269531, 5.246288299560547, 0.0]}]}